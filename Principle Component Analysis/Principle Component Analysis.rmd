---
title: "Principle Component Analysis"
author: "Jainendra Upreti"
date: "July 16, 2017"
output: 
  html_document:
    df_print: paged
    toc: true
---

```{r echo=FALSE, warning=FALSE, results='hide', message=FALSE}
library(tibble)
library(tidyr)
library(ggplot2)
#library(purrr)
library(dplyr)
library(knitr)
library(RCurl) # To download data from URL
library(caret) # nearZeroVar function
library(xgboost)
```

## Introduction

This document deals with:

1) the problems faced with dimensionality.
2) The ways in which we can deal with redundancy and multicollinearity introduced in our dataset due to high dimensionality.
3) Detecting multicollinearity.
4) Remedies for multicollinearity and dimension reduction using Principle Component Analysis(PCA).
5) PCA Implemtation example with R-codes

## Dimensionality and related issues

Dimensionality of the data is defined by the number of independent, predictor, or explanatory variables in the dataset. Having said that, it is often observed that dimensionality effects the computational performance more than the number of observations. Not only it increases the complexity of the data set, but it also limits the ability to explore and model relationships between variables.

## Cause of dimensionality

Two factors can contribute towards high dimenstionaly. First, Having categorical variables with various levels and second, redundant variables in a dataset. When we include categorical variables in modeling we use dummy variables, and more the number of levels in a categorical variable, the more dummy variables we create, which inturn increases the dimensionality. This can be solved by an approach known as "Collapsing the levels", which will be disucssed in a separate document. This document will focus on dealing with redundnant variables and multicollinearity.

Multicollinearity (collinearity) can be defined as a phenomenon in which two or more variables in a multivariate regression are highly correlated, that means, one variable can be linearly predicted from the others with a substantial degree of accuracy. Collinear variables contain the same information about the dependent variable. If nominally "different" measures actually quantify the same phenomenon, then they are reduntant variables.

Presence of redundant variables and multicollinearity among variables can effect the analyses of variables in many ways, some of which are shared below:

* Destabilizing the parameter estimates
* Confounding model interpretions
* Increasing risk of overfitting
* Increasing computation time

Small changes in input data (like removing or adding a single variable) leads to a large change in the model, even resulting in sign changes of parameters. Also, the presence of multicollinearity increases the variance or standard error of the coefficient estimates making it sensitive to minor changes, thus resulting in difficulty in interpretation.

When we talk about best regression model for a dataset, it means that the model considers predictor variables where each predictor correlates with the target/dependent variable but correlate almost minimally with each other. Such a model is called low-noise model and is stastically robust and can be used for scoring Now, if the correlation among the variables itself is high and we achieve high predictive power on the training dataset, it is possible that we may not achieve the same on the test dataset, unless the same collinear relation exists between the variables in test dataset, in which case multicollinearity won't effect the predictive ability of your model.

## Detecting Multicollinearity

After understanding how how multicollinearity affects our analysis and predictive ability of the model, it is very important to learn how to detect the presence of multicollinearity in our data.

Following methods can be used to detect multicollinearity:

* The analysis exhibits sign of multicollinearity - such as, coefficient estimates varying from model to model
* The R-square value is large but none of the beta weights is statistically significant, i.e., F-test for overall model is significant but the t-tests for individual coefficient estimates are not.
* Correlation among pairs of variables are large.
* Variance Inflation Factor.

### Variable Inflation Factor (VIF)

One of the points that was mentioned in detecting multicollinearity was, that the correlation between pairs of variables are large, but sometimes looking only at the correlation among pairs of predictors can be a limiting factor.

It is possible that pairwise correlation are small and yet there exists a linear dependece between three or more variables. For example, the relation between X1, X2, and X3 is defined by a linear equation, say: X3 = 3X1 + 4X2 + Error. For such cases, we use Variance Inflation Factor.

VIF is a measure of how much the variance of the estimated regression coefficient, can be inflated by the existence of correlations among the predictor variables in the model. If the value of VIF > 10, it shows that there is some serious correlation between variables.

We won't be dwelve into mathematical derivations for VIF. The formula for VIF is given as

                    VIF = 1/(1-R-square)

VIF for a predictor variable "k" is obtained by calculating R squared value by regressing that k-th variable on the remaining predictor variable.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Remedies for multicollinearity and high dimensionality

* Check if one of the variables is a duplicate
* Drop a redundant variable
* Increasing the sample size by collecting more data
* Mean centering the predictor variables
* Standardization of predictor variable - if mean centering has no affect
* Principle Component Analysis, Ridge regression, partial least squares regression

In this document, we will discuss Principle Component Analysis with the help of an example.

### Principle Component Analysis (PCA)

PCA is also called as feature reduction or feature extraction. PCA creates new features using the many features already present in the dataset. In PCA, instead of regressing the dependent variable on the explanatory variables directly, the principal components of the explanatory variables are used as regressors. One typically uses only a subset of all the principal components for regression, thus making PCR some kind of a regularized procedure. Often the principal components with higher variances are selected as regressors.

One major use of PCA lies in overcoming the multicollinearity problem. PCA can aptly deal with such situations by excluding some of the low-variance principal components in the regression step. In addition, by usually regressing on only a subset of all the principal components, PCR can result in dimension reduction through substantially lowering the effective number of parameters characterizing the underlying model.

### Learning PCA through Example

To understaind how PCA works, the first thing we need a dataset with high dimensionality. This data can be downloaded using the code below:

```{r warning=FALSE, message=FALSE}
# donwloading the file1
urlfile <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/gisette/GISETTE/gisette_train.data'
x <- getURL(urlfile, ssl.verifypeer = FALSE)
gisetteRaw <- read.table(textConnection(x), sep = '', header = FALSE, stringsAsFactors = FALSE)

# download file2
urlfile <- "https://archive.ics.uci.edu/ml/machine-learning-databases/gisette/GISETTE/gisette_train.labels"
x <- getURL(urlfile, ssl.verifypeer = FALSE)
g_labels <- read.table(textConnection(x), sep = '', header = FALSE, stringsAsFactors = FALSE)

print(dim(gisetteRaw))
head(gisetteRaw)

```

On running a check on dimension for our data, we can observe that this data has 6000 observations distributed over 5000 predictor variables.

As discussed above, PCA creates new features using the many features present in the dataset. A problem that may arise with PCA is that if the variables have zero variance or very little variance, i.e., almost the same values throughtout all observations, the PCA collapses. Also, thinking about statistical relevance of such a data, it can be said that a variable of almost zero variance won't contribute much to the model.

